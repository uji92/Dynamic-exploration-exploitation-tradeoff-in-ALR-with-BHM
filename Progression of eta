

toteta = []

inst = [100] # number of data to include in training set


for h in range(len(inst)):
        print(inst[h])
        MSE = np.zeros((1,1))
        for NIt in range(1):
            X = t[random.randint(0,(len(t)-1))].reshape(-1,1)
            X = np.append(X, t[random.randint(0,(len(t)-1))]).reshape(-1,1) #randomly include a few training data


            Y = []
            for j in X:
                y = Ytrue(j)
                Y = np.append(Y,y)  #randomly include a few training data


            tm = (np.linspace(0,50,1000)).reshape(-1, 1)
            obs = 1
            Nt = tm.size
            t = tm
            max_iter = inst[h]
            for iter in range(max_iter):
                N = X.size
                x0 = 1
                res = minimize(M32, x0, bounds=np.array([[1, 300]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx =kM32(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx = kM32(t, X,ell1)
                ktt = kM32(t,t,ell1) +(((10**(-6)))*np.eye(Nt))

                alpha = solve(kxx, ktx.T).T
                mpost = alpha @ Y
                vpost = ktt - alpha @ ktx.T  # posterior covariance
                spost = np.sqrt( np.diag(vpost))
                spost = np.nan_to_num(spost)

## Exploration
                Explore = np.zeros((len(t),1))
                for j in range(len(t)):
                    dx = np.abs(t[j]-X)
                    dyr = np.abs(mpost[j]-Y) # Y-norm with the X  that has the max x-norm
                    dzr = dyr*dx # ratio of y-norm to x-norm
                    Explore[j,0] = np.min(dzr)

## Exploitation
                x0 = 1
                res = minimize(SE, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx1 =kSE(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx1 = kSE(t, X,ell1)
                alpha1 = solve(kxx1, ktx1.T).T
                mpost1 = alpha1 @ Y


                kxx2 =kE(X, X,1.5*10**6) +(((10**(-6)))*np.eye(N))
                ktx2 = kE(t, X,1.5*10**6)
                alpha2 = solve(kxx2, ktx2.T).T
                mpost2 = alpha2 @ Y

                x0 = 1
                res = minimize(M32, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx3 =kM32(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx3 = kM32(t, X,ell1)
                alpha3 = solve(kxx3, ktx3.T).T
                mpost3 = alpha3 @ Y


                x0 = 1
                res = minimize(M52, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx4 =kM52(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx4 = kM52(t, X,ell1)
                alpha4 = solve(kxx4, ktx4.T).T
                mpost4 = alpha4 @ Y



                Exploite = np.zeros((len(t),1))
                for i in range(len(t)):
                  ss = np.zeros((4,1))
                  ss[0] = mpost1[i]
                  ss[1] = mpost2[i]
                  ss[2] = mpost3[i]
                  ss[3] = mpost4[i]
               
                  Exploite[i,0] = np.max(ss)-np.min(ss)
                Exploite = Exploite/np.sum(Exploite)
                Explore = Explore/np.sum(Explore)

## Metropolis within Gibbs
                a0 = np.random.uniform(0.1,5,1) #2
                b0 = np.random.uniform(0.1,5,1) #1
                eta0 = np.random.beta(a0,b0)
                # print("initial eta")
                # print(eta0)
                Finaleta = np.zeros((50))
                Finaleta[0] = eta0

                for v in range(50):
                    a1 = a0
                    b1 = b0
                    a0 = minimize(beta_a, a1, bounds=np.array([[0.1, 5]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True}).x
                    b0 = minimize(beta_b, b1, bounds=np.array([[0.1, 5]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True}).x
                    for l in range(30):

                        # eta0 = np.random.beta(a0,b0)

                        rnde = np.random.normal(eta0,0.1)
                        # xurnd = np.argmax((rnde*Explore)+((1-rnde)*Exploite))
                        # # xog = np.argmax((eta0*Explore)+((1-eta0)*Exploite))


                        # res1 = minimize(M32, 1, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                        # l1 = res1.x

                        # kxx = kM32(X, X,l1) +(((10**(-6)))*np.eye(N))
                        # ktx = kM32(t[xurnd], X,l1)
                        # ktt = kM32(t[xurnd],t[xurnd],l1) #+(((10**(-6)))*np.eye(N+1))
                        # alpha = solve(kxx, ktx.T).T

                        # vpost = ktt - alpha @ ktx.T  # posterior covariance
                        # if vpost >= 0.001:
                        re2 = scipy.stats.beta.pdf(rnde,a0,b0)/scipy.stats.beta.pdf(eta0,a0,b0)
                        # re = vpost*re2
                        if re2>=0.85:
                          e0 = rnde
                        else:
                          e0 = eta0
                        # else:
                        #   e0 = eta0

                    eta0 = e0
                    Finaleta[v]=eta0


                Etastar = np.average(Finaleta[30:])
                print(Etastar)

                FinalObj = ((Etastar*Explore) + ((1-Etastar)*Exploite))
                nxt =  np.argmax(FinalObj)


                X = (np.append(X, t[nxt])).reshape(-1, 1) # Add the point to existing dataset
                Y = (np.append(Y,Ytrue(t[nxt]))) # Add the label
                N = X.size



            toteta.append(Etastar)
            print(Etastar)
font = {'family': 'Liberation serif',
        'serif': 'Times',
        'weight': 'normal',
        'size': 12}
plt.pyplot.rc('font', **font)
plt.pyplot.plot(etas,linestyle='--',color='blue',linewidth=1)
plt.pyplot.scatter(np.linspace(0,99,100),etas1,c='red')
