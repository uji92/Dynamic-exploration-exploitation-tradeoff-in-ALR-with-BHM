## import packages

import numpy as np
import scipy
from numpy.linalg import cholesky, solve
from numpy.random import random
from scipy.linalg import cho_solve, cho_factor
import matplotlib as plt
import pandas as pd
import io
from numpy import inf
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
from scipy.optimize import minimize
import time
import random
from scipy.stats import beta
from scipy.stats import norm

## Tha blackbox function and true values
ytrue = []
x = np.linspace (0,50,1000)
for i in x:
    if i < 25:
        yt = 3.5*np.exp(-((i-10)**2)/200)
    else:
        yt = 8-3.5*(np.exp(-((i-35)**2)/200))
    
    ytrue.append(yt)

plt.pyplot.plot(x,ytrue)

## Noisy observed data
sigma = 0.12  # data noise
def Ytrue(x):
    if x <25:
        return (3.5*np.exp(-((x-10)**2)/200))+ (np.random.normal(0,np.sqrt(sigma)))
    else:
        return (8-3.5*(np.exp(-((x-35)**2)/200)))+ (np.random.normal(0,np.sqrt(sigma)))    
X = (np.linspace(0,50,5).reshape(-1,1))



## Different kernel functions and their likelihood
def kSE(a,b,l):
    sqdist = (a-b.T)**2
    return np.exp(-sqdist/(2*(l**2)))
def kE(a,b,l):
    sqdist = np.abs(a-b.T)
    return np.exp(-sqdist/(2*l))
def kM32(a,b,l):
    r = np.abs((a - b.T))
    r[r == 0] = 0.00000001
    part1 = 1+(np.sqrt(3)*r/l)
    part2 = np.exp(-(np.sqrt(3)*r/l))
    return (part1 * part2)
def kM52(a,b,l):
    r = np.abs((a - b.T))
    r[r == 0] = 0.00000001
    part1 = 1+(np.sqrt(5)*r/l)+((5*(r**2))/(3*(l**2)))
    part2 = np.exp(-(np.sqrt(5)*r/l))
    return (part1 * part2)
def k121 (a,b,l):
    part1 = kSE(a,b,l)
    part2 = kE(a,b,l)
    return part1*part2
def k122 (a,b,l):
    part1 = kSE(a,b,l)
    part2 = kE(a,b,l)
    return part1+part2
def k341(a,b,l):
    part1 = kM32(a,b,l)
    part2 = kM52(a,b,l)
    return part1*part2
def k342(a,b,l):
    part1 = kM32(a,b,l)
    part2 = kM52(a,b,l)
    return art1+part2
def kM32E1(a,b,l1,l2):
    part1 = kM32(a,b,l1)
    part2 = kE(a,b,l2)
    return part1*part2
def kM32E2(a,b,l1,l2):
    part1 = kM32(a,b,l1)
    part2 = kE(a,b,l2)
    return part1+part2
def kdot(a,b,l):
    cov = a @ b.T 
    return l + cov
def SE(l):
    kxx = kSE(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                   # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def E(l):
    kxx = kE(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                   # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def M32(l):
    kxx = kM32(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                             # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def M52(l):
    kxx = kM52(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                             # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def dot(l):
    kxx = kdot(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                             # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
    
    


mean = np.zeros((3,1))
totprop = np.zeros((30,3))

inst = [1,3,5,7,9,11,13,15,17,19,21,23] # number of data to include in training set


for h in range(len(inst)):
        print(inst[h])
        MSE = np.zeros((30,1))
        for NIt in range(30):
            X = t[random.randint(0,(len(t)-1))].reshape(-1,1)  
            X = np.append(X, t[random.randint(0,(len(t)-1))]).reshape(-1,1) #randomly include a few training data
            

            Y = []
            for j in X:
                y = Ytrue(j)
                Y = np.append(Y,y)  #randomly include a few training data
            
            
            tm = (np.linspace(0,50,1000)).reshape(-1, 1)
            obs = 1
            Nt = tm.size
            t = tm
            max_iter = inst[h]
            for iter in range(max_iter):
                N = X.size
                x0 = 1
                res = minimize(M32, x0, bounds=np.array([[1, 300]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx =kM32(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx = kM32(t, X,ell1)
                ktt = kM32(t,t,ell1) +(((10**(-6)))*np.eye(Nt))

                alpha = solve(kxx, ktx.T).T
                mpost = alpha @ Y
                vpost = ktt - alpha @ ktx.T  # posterior covariance
                spost = np.sqrt( np.diag(vpost))
                spost = np.nan_to_num(spost)

## Exploration
                Explore = np.zeros((len(t),1))
                for j in range(len(t)):
                    dx = np.abs(t[j]-X)
                    dyr = np.abs(mpost[j]-Y) # Y-norm with the X  that has the max x-norm
                    dzr = dyr*dx # ratio of y-norm to x-norm
                    Explore[j,0] = np.min(dzr)         
                
## Exploitation
                x0 = 1
                res = minimize(SE, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx1 =kSE(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx1 = kSE(t, X,ell1)
                alpha1 = solve(kxx1, ktx1.T).T
                mpost1 = alpha1 @ Y


                kxx2 =kE(X, X,1.5*10**6) +(((10**(-6)))*np.eye(N))
                ktx2 = kE(t, X,1.5*10**6)
                alpha2 = solve(kxx2, ktx2.T).T
                mpost2 = alpha2 @ Y

                x0 = 1
                res = minimize(M32, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx3 =kM32(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx3 = kM32(t, X,ell1)
                alpha3 = solve(kxx3, ktx3.T).T
                mpost3 = alpha3 @ Y
                

                x0 = 1
                res = minimize(M52, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx4 =kM52(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx4 = kM52(t, X,ell1)
                alpha4 = solve(kxx4, ktx4.T).T
                mpost4 = alpha4 @ Y

               

                x0 = 1
                res = minimize(dot, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx8 =kdot(X,X,10**2)+1000 +(((10**(-6)))*np.eye(N))
                ktx8 = kdot(t,X,10**2)+1000
                alpha8 = solve(kxx8, ktx8.T).T
                mpost8 = alpha8 @ Y

                kxx9 =(kM52(X, X,2*10**9))*(kE(X, X,10**6))+(((10**(-6)))*np.eye(N))
                ktx9 =(kM52(t, X,2*10**9))*(kE(t, X,10**6))
                alpha9 = solve(kxx9, ktx9.T).T
                mpost9 = alpha9 @ Y

                kxx10 = kM52(X, X,50)*kM32(X,X,10**8)*kE(X,X,8*10**8)+(((10**(-6)))*np.eye(N))
                ktx10 =kM52(t, X,50)*kM32(t,X,10**8)*kE(t,X,8*10**8)
                alpha10 = solve(kxx10, ktx10.T).T
                mpost10 = alpha10 @ Y


                Exploite = np.zeros((len(t),1))
                for i in range(len(t)):
                  ss = np.zeros((7,1))
                  ss[0] = mpost1[i]
                  ss[1] = mpost2[i]
                  ss[2] = mpost3[i]
                  ss[3] = mpost4[i]
                  # ss[4] = mpost5[i]
                  # ss[5] = mpost6[i]
                  # ss[6] = mpost7[i]
                  ss[4] = mpost8[i]
                  ss[5] = mpost9[i]
                  ss[6] = mpost10[i]
                  Exploite[i,0] = np.max(ss)-np.min(ss)
                Exploite = Exploite/np.sum(Exploite)  
                Explore = Explore/np.sum(Explore)  
                
## Metropolis within Gibbs
                a0 = np.random.uniform(0.1,5,1) #2
                b0 = np.random.uniform(0.1,5,1) #1
                eta0 = np.random.beta(a0,b0)
                # print("initial eta")
                # print(eta0)
                Finaleta = np.zeros((1000))

                for v in range(1000):
                    k = np.linspace(0.1,5, 30)
                    eps = np.zeros((1))
                    eta = (np.linspace(0,1, 10).reshape(-1,1)).T
                    for n in range(100):

                        rnda = np.random.normal(a0,np.minimum(np.abs(a0-0.1)/3,np.abs(a0-5)/3))
                      
                        eta0 = np.random.beta(a0,b0)
                        ra = scipy.stats.beta.pdf(eta0,rnda,b0)/scipy.stats.beta.pdf(eta0,a0,b0)
                        
                        if ra>=0.5:
                          a1 = rnda
                        else:
                          a1 = a0
                    a0 = a1 #a[n] #np.sum(a*posta)
                    
                    for m in range(100):
                        rndb = np.random.normal(b0,np.minimum(np.abs(b0-0.1)/3,np.abs(b0-5)/3))
                        
                        eta0 = np.random.beta(a0,b0)
                        rb = scipy.stats.beta.pdf(eta0,a0,rndb)/scipy.stats.beta.pdf(eta0,a0,b0)
                        
                        if rb>=0.5:
                          b1 = rndb
                        else:
                          b1 = b0
                    b0 = b1 #b[m]  #  np.sum(b*postb)
                    for l in range(100):
                        
                        eta0 = np.random.beta(a0,b0)
                       
                        rnde = np.random.normal(eta0,np.minimum(np.abs(eta0)/3.33,np.abs(eta0-1)/3.33))
                        xurnd = np.argmax((rnde*Explore)+((1-rnde)*Exploite))
                        xog = np.argmax((eta0*Explore)+((1-eta0)*Exploite))
  
                        
                        res1 = minimize(M32, 1, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                        l1 = res1.x

                        kxx = kM32(X, X,l1) +(((10**(-6)))*np.eye(N))
                        ktx = kM32(t[xurnd], X,l1)
                        ktt = kM32(t[xurnd],t[xurnd],l1) #+(((10**(-6)))*np.eye(N+1))
                        alpha = solve(kxx, ktx.T).T
                
                        vpost = ktt - alpha @ ktx.T  # posterior covariance
                        if vpost >= 0.001:
                          re2 = scipy.stats.beta.pdf(rnde,a0,b0)/scipy.stats.beta.pdf(eta0,a0,b0)
                          re = vpost*re2
                          if re>=1:
                            e0 = rnde
                          else:
                            e0 = eta0
                        else:
                          e0 = eta0
                        
                    eta0 = e0 
                    Finaleta[v]=eta0
 

                Etastar = np.average(Finaleta[300:])
                

                FinalObj = ((Etastar*Explore) + ((1-Etastar)*Exploite))
                nxt =  np.argmax(FinalObj)
                

                X = (np.append(X, t[nxt])).reshape(-1, 1) # Add the point to existing dataset
                Y = (np.append(Y,Ytrue(t[nxt]))) # Add the label
                N = X.size
            

            x0 = 1
            res = minimize(M32, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
            ell = res.x 
            
            kxx10 = kM32(X,X,ell)+(((10**(-6)))*np.eye(N))
            ktx10 =kM32(t,X,ell)
            alpha10 = solve(kxx10, ktx10.T).T
            mpost = alpha10 @ Y
            
            err = (ytrue-mpost)**2
           
            MSE[NIt] = np.sqrt(np.average(err))
            totprop[NIt,h] = np.sqrt(np.average(err))
            print(totprop[NIt,h])
            
            
        mean[h] = np.average(MSE)
        print(mean[h])
        

totprop    
