## import packages

import numpy as np
import scipy
from numpy.linalg import cholesky, solve
from numpy.random import random
from scipy.linalg import cho_solve, cho_factor
import matplotlib as plt
import pandas as pd
import io
from numpy import inf
%matplotlib inline
%config InlineBackend.figure_format = 'retina'
from scipy.optimize import minimize
import time
import random
from scipy.stats import beta
from scipy.stats import norm

## Tha blackbox function and true values
ytrue = []
x = np.linspace (0,50,1000)
for i in x:
    if i < 25:
        yt = 3.5*np.exp(-((i-10)**2)/200)
    else:
        yt = 8-3.5*(np.exp(-((i-35)**2)/200))
    
    ytrue.append(yt)

plt.pyplot.plot(x,ytrue)

## Noisy observed data
sigma = 0.12  # data noise
def Ytrue(x):
    if x <25:
        return (3.5*np.exp(-((x-10)**2)/200))+ (np.random.normal(0,np.sqrt(sigma)))
    else:
        return (8-3.5*(np.exp(-((x-35)**2)/200)))+ (np.random.normal(0,np.sqrt(sigma)))    
X = (np.linspace(0,50,5).reshape(-1,1))



## Different kernel functions and their likelihood
def kSE(a,b,l):
    sqdist = (a-b.T)**2
    return np.exp(-sqdist/(2*(l**2)))
def kE(a,b,l):
    sqdist = np.abs(a-b.T)
    return np.exp(-sqdist/(2*l))
def kM32(a,b,l):
    r = np.abs((a - b.T))
    r[r == 0] = 0.00000001
    part1 = 1+(np.sqrt(3)*r/l)
    part2 = np.exp(-(np.sqrt(3)*r/l))
    return (part1 * part2)
def kM52(a,b,l):
    r = np.abs((a - b.T))
    r[r == 0] = 0.00000001
    part1 = 1+(np.sqrt(5)*r/l)+((5*(r**2))/(3*(l**2)))
    part2 = np.exp(-(np.sqrt(5)*r/l))
    return (part1 * part2)
def k121 (a,b,l):
    part1 = kSE(a,b,l)
    part2 = kE(a,b,l)
    return part1*part2
def k122 (a,b,l):
    part1 = kSE(a,b,l)
    part2 = kE(a,b,l)
    return part1+part2
def k341(a,b,l):
    part1 = kM32(a,b,l)
    part2 = kM52(a,b,l)
    return part1*part2
def k342(a,b,l):
    part1 = kM32(a,b,l)
    part2 = kM52(a,b,l)
    return art1+part2
def kM32E1(a,b,l1,l2):
    part1 = kM32(a,b,l1)
    part2 = kE(a,b,l2)
    return part1*part2
def kM32E2(a,b,l1,l2):
    part1 = kM32(a,b,l1)
    part2 = kE(a,b,l2)
    return part1+part2
def kdot(a,b,l):
    cov = a @ b.T 
    return l + cov
def SE(l):
    kxx = kSE(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                   # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def E(l):
    kxx = kE(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                   # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def M32(l):
    kxx = kM32(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                             # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def M52(l):
    kxx = kM52(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                             # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
def dot(l):
    kxx = kdot(X, X, l)   +((10**(-6))*np.eye(N))              # K + (sigma^2)I
    Q = cholesky(kxx)                                   # kxx1 = Q @ Q.T
    model_complexity =  np.sum(np.log(np.diagonal(Q)))   # 0.5 * np.log(det(kxx1))
    P = scipy.linalg.cho_solve((Q, True), Y)
    data_fit = 0.5*(np.dot(Y.T, P))                             # 0.5 * (Y.T @ inv(kxx1) @ Y)
    return model_complexity + data_fit + (0.5 * len(X) * np.log(2*np.pi))
    
    


mean = np.zeros((3,1))
totent = np.zeros((30,3))

inst = [1,3,5,7,9,11,13,15,17,19,21,23] # number of data to include in training set


for h in range(len(inst)):
        print(inst[h])
        MSE = np.zeros((30,1))
        for NIt in range(30):
            X = t[random.randint(0,(len(t)-1))].reshape(-1,1)  
            X = np.append(X, t[random.randint(0,(len(t)-1))]).reshape(-1,1) #randomly include a few training data
            

            Y = []
            for j in X:
                y = Ytrue(j)
                Y = np.append(Y,y)  #randomly include a few training data
            
            
            tm = (np.linspace(0,50,1000)).reshape(-1, 1)
            obs = 1
            Nt = tm.size
            t = tm
            max_iter = inst[h]
            for iter in range(max_iter):
                N = X.size
                x0 = 1
                res = minimize(M32, x0, bounds=np.array([[1, 300]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
                ell1 = res.x
                kxx =kM32(X, X,ell1) +(((10**(-6)))*np.eye(N))
                ktx = kM32(t, X,ell1)
                ktt = kM32(t,t,ell1) +(((10**(-6)))*np.eye(Nt))

                alpha = solve(kxx, ktx.T).T
                mpost = alpha @ Y
                vpost = ktt - alpha @ ktx.T  # posterior covariance
                spost = np.sqrt( np.diag(vpost))
                spost = np.nan_to_num(spost)
                ent = scipy.stats.entropy(scipy.stats.norm.pdf(t,mpost,spost))

## Max entropy
                       
                
                FinalObj = ent
                nxt =  np.argmax(FinalObj)
                

                X = (np.append(X, t[nxt])).reshape(-1, 1) # Add the point to existing dataset
                Y = (np.append(Y,Ytrue(t[nxt]))) # Add the label
                N = X.size
            

            x0 = 1
            res = minimize(M32, x0, bounds=np.array([[1, 500]]), method='L-BFGS-B', options= {'ftol': 1e-10, 'disp': True})
            ell = res.x 
            
            kxx10 = kM32(X,X,ell)+(((10**(-6)))*np.eye(N))
            ktx10 =kM32(t,X,ell)
            alpha10 = solve(kxx10, ktx10.T).T
            mpost = alpha10 @ Y
            
            err = (ytrue-mpost)**2
           
            MSE[NIt] = np.sqrt(np.average(err))
            totent[NIt,h] = np.sqrt(np.average(err))
            print(totent[NIt,h])
            
            
        mean[h] = np.average(MSE)
        print(mean[h])
        

totent 
